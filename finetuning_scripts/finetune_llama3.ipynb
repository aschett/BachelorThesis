{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from transformers import (AutoTokenizer, TrainingArguments)\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "output_dir = '../finetuned_models/outputmodel_standard/llama3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Preprocess the Dataset\n",
    "\n",
    "First, we load the dataset from `quotes.csv` and preprocess it for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "try:\n",
    "    file_path = '../dataset/quotes_classification_data.csv'\n",
    "    dataset = pd.read_csv(file_path)\n",
    "    print('Dataset lodaded succesfully')\n",
    "\n",
    "    if 'Memorable' in dataset.columns:\n",
    "        # Convert 'Memorable' column to binary values\n",
    "        dataset['Memorable'] = dataset['Memorable'].map({'Yes': 1, 'No': 0})\n",
    "        print('Memorable column converted to binary values.')\n",
    "    else:\n",
    "        raise KeyError('Memorable column not found in dataset')\n",
    "\n",
    "    if dataset['Memorable'].isnull().any():\n",
    "        raise ValueError('Memorable column contains missing values.')\n",
    "      \n",
    "    # Split dataset into train and validation sets\n",
    "    train_df, val_df = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convert to Huggingface datasets, ensuring correct types\n",
    "    train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "    val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file at {file_path} was not found. Please check the file path.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: The CSV file is empty or corrupt.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    # Catch oll other Exceptions which could occur\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tokenize the Dataset\n",
    "\n",
    "Use tokenizer to preprocess the data so it fits our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    if 'Quote' not in examples:\n",
    "        raise KeyError('Error: Column Quote not found in Dataset.')\n",
    "    return tokenizer(examples[\"Quote\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"Memorable\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"Memorable\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Fine-Tune the Model\n",
    "\n",
    "We'll fine-tune the pre trained model using the tokenized dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512 # Depends on the length of the quotes, as no quotes are very long, 512 should be more than sufficient\n",
    "\n",
    "number_of_training_examples = len(train_dataset)\n",
    "per_device_train_batch_size = 8\n",
    "gradient_accumulation_steps = 2\n",
    "per_device_eval_size = 4\n",
    "number_of_devices = 1\n",
    "learning_rate = 3e-5\n",
    "warmup_steps = 100\n",
    "\n",
    "steps_per_epoch = (number_of_training_examples // (per_device_train_batch_size * gradient_accumulation_steps * number_of_devices))\n",
    "number_of_epochs = 30\n",
    "\n",
    "try:\n",
    "    model, _ = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True\n",
    "    )\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error loading model {model_name}: {e}\")\n",
    "\n",
    "# Apply fast LoRA weights and model patching\n",
    "try:\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=3407,\n",
    "        max_seq_length=max_seq_length,\n",
    "        use_rslora=False,\n",
    "        loftq_config=None,\n",
    "    )\n",
    "    print(\"LoRA weights applied successfully.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error applying LoRA weights to the model: {e}\")\n",
    "\n",
    "# Define training arguments\n",
    "try:\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        per_device_eval_batch_size=per_device_eval_size,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_steps=warmup_steps,\n",
    "        max_steps=steps_per_epoch * number_of_epochs,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=50,\n",
    "        output_dir=output_dir,\n",
    "        optim=\"adamw_8bit\",\n",
    "        seed=3407\n",
    "    )\n",
    "    print(\"Training arguments initialized successfully.\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error in initializing training arguments: {e}\")\n",
    "\n",
    "# Initialize the trainer\n",
    "try:\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        dataset_text_field='Quote',\n",
    "    )\n",
    "    print(\"Trainer initialized successfully.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error initializing trainer: {e}\")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Clean up and Saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fine-tuned model and tokenizer\n",
    "try:\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(\"Tokenizer and model saved successfully.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error saving model or tokenizer to {output_dir}: {e}\")\n",
    "\n",
    "# Save the validation dataset\n",
    "try:\n",
    "    # Ensure output directory exists and if not create it\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Save validation dataset as CSV\n",
    "    val_df.to_csv(os.path.join(output_dir, 'val_dataset.csv'), index=False)\n",
    "    print(\"Validation dataset saved successfully.\")\n",
    "except PermissionError:\n",
    "    raise PermissionError(f\"Permission denied: unable to write to {output_dir}.\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Directory {output_dir} not found or cannot be created.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error saving validation dataset to {output_dir}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
