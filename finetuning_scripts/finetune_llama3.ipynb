{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np._import_array() \n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from transformers import (AutoTokenizer, TrainingArguments)\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "output_dir = '../finetuned_models/outputmodel_standard/llama3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Preprocess the Dataset\n",
    "\n",
    "First, we load the dataset from `quotes.csv` and preprocess it for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = '../dataset/quotes_classification_data.csv'\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'Memorable' column to binary values\n",
    "dataset['Memorable'] = dataset['Memorable'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_df, val_df = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to Huggingface datasets, ensuring correct types\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tokenize the Dataset\n",
    "\n",
    "Use tokenizer to preprocess the data so it fits our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"Quote\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"Memorable\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"Memorable\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Fine-Tune the Model\n",
    "\n",
    "We'll fine-tune the pre trained model using the tokenized dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512 # Depends on the length of the quotes, as no quotes are very long, 512 should be more than sufficient\n",
    "\n",
    "number_of_training_examples = len(train_dataset)\n",
    "per_device_train_batch_size = 8\n",
    "gradient_accumulation_steps = 2\n",
    "per_device_eval_size = 4\n",
    "number_of_devices = 1\n",
    "learning_rate = 3e-5\n",
    "warmup_steps = 100\n",
    "\n",
    "steps_per_epoch = (number_of_training_examples // (per_device_train_batch_size * gradient_accumulation_steps * number_of_devices))\n",
    "number_of_epochs = 10\n",
    "\n",
    "model, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# Apply fast LoRA weights and model patching\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    max_seq_length=max_seq_length,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=per_device_eval_size,\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    max_steps=steps_per_epoch * number_of_epochs,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=50,\n",
    "    output_dir=output_dir,\n",
    "    optim=\"adamw_8bit\",\n",
    "    seed=3407\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    dataset_text_field='Quote',\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Clean up and Saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Save the validation dataset\n",
    "val_df.to_csv(os.path.join(output_dir, 'val_dataset.csv'), index=False)\n",
    "\n",
    "print(\"Model, tokenizer, and validation dataset saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
